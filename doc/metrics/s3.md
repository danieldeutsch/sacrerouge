# S3
S3 is a learned metric for summary content quality as proposed in [1].

## Setting Up
The implementation in SacreROUGE is using [our fork](https://github.com/danieldeutsch/S3) of the [original repository](https://github.com/UKPLab/emnlp-ws-2017-s3).
Our fork adds functionality to run the metric on a collection of (summary, references) pairs.

The code is uses Python 2.7, so the conda environment name needs to be passed to the constructor of the `S3` class in order for the code to run.
The requirements for S3 are:
```
numpy==1.12.1
nltk==3.2.1
scipy==0.19.0
six
click
scikit-learn==0.18.1
```
Then, the NLTK stopwords must also be downloaded:
```
python -m nltk.downloader stopwords
```

To download the dependencies for S3, run the following command:
```
sacrerouge setup-metric s3
```

To verify your installation, set the environment variable `S3_ENV` to be the name of the conda environment, then run:
```
pytest sacrerouge/tests/metrics/s3_test.py
```

## Correlations
Here are the correlations of the two different S3 scores to the "overall responsiveness" human judgments on several datasets.
It is not clear what dataset(s) the parameters of S3 were trained on, but the paper reports results on both TAC 2008 and 2009.

Summary-level, peers only:
<table>
<tr>
<th></th>
<th colspan="3">TAC2008</th>
<th colspan="3">TAC2009</th>
<th colspan="3">TAC2010</th>
<th colspan="3">TAC2011</th>
</tr>
<tr>
<th></th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
</tr>
<tr>
<td>S3-Pyr</td>
<td>0.51</td>
<td>0.50</td>
<td>0.41</td>
<td>0.56</td>
<td>0.50</td>
<td>0.40</td>
<td>0.69</td>
<td>0.66</td>
<td>0.55</td>
<td>0.59</td>
<td>0.53</td>
<td>0.42</td>
</tr>
<tr>
<td>S3-Resp</td>
<td>0.51</td>
<td>0.50</td>
<td>0.40</td>
<td>0.53</td>
<td>0.51</td>
<td>0.40</td>
<td>0.69</td>
<td>0.66</td>
<td>0.54</td>
<td>0.57</td>
<td>0.52</td>
<td>0.42</td>
</tr>
</table>

Summary-level, peers + references:
<table>
<tr>
<th></th>
<th colspan="3">TAC2008</th>
<th colspan="3">TAC2009</th>
<th colspan="3">TAC2010</th>
<th colspan="3">TAC2011</th>
</tr>
<tr>
<th></th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
</tr>
<tr>
<td>S3-Pyr</td>
<td>0.59</td>
<td>0.56</td>
<td>0.46</td>
<td>0.56</td>
<td>0.56</td>
<td>0.44</td>
<td>0.73</td>
<td>0.72</td>
<td>0.59</td>
<td>0.59</td>
<td>0.54</td>
<td>0.44</td>
</tr>
<tr>
<td>S3-Resp</td>
<td>0.58</td>
<td>0.56</td>
<td>0.45</td>
<td>0.52</td>
<td>0.55</td>
<td>0.44</td>
<td>0.72</td>
<td>0.71</td>
<td>0.58</td>
<td>0.56</td>
<td>0.53</td>
<td>0.42</td>
</tr>
</table>

System-level, peers only:
<table>
<tr>
<th></th>
<th colspan="3">TAC2008</th>
<th colspan="3">TAC2009</th>
<th colspan="3">TAC2010</th>
<th colspan="3">TAC2011</th>
</tr>
<tr>
<th></th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
</tr>
<tr>
<td>S3-Pyr</td>
<td>0.81</td>
<td>0.82</td>
<td>0.61</td>
<td>0.83</td>
<td>0.79</td>
<td>0.62</td>
<td>0.95</td>
<td>0.95</td>
<td>0.85</td>
<td>0.91</td>
<td>0.78</td>
<td>0.60</td>
</tr>
<tr>
<td>S3-Resp</td>
<td>0.81</td>
<td>0.82</td>
<td>0.62</td>
<td>0.79</td>
<td>0.81</td>
<td>0.64</td>
<td>0.95</td>
<td>0.95</td>
<td>0.83</td>
<td>0.89</td>
<td>0.78</td>
<td>0.60</td>
</tr>
</table>

System-level, peers + references:
<table>
<tr>
<th></th>
<th colspan="3">TAC2008</th>
<th colspan="3">TAC2009</th>
<th colspan="3">TAC2010</th>
<th colspan="3">TAC2011</th>
</tr>
<tr>
<th></th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
</tr>
<tr>
<td>S3-Pyr</td>
<td>0.87</td>
<td>0.88</td>
<td>0.70</td>
<td>0.72</td>
<td>0.85</td>
<td>0.69</td>
<td>0.91</td>
<td>0.97</td>
<td>0.87</td>
<td>0.77</td>
<td>0.84</td>
<td>0.67</td>
</tr>
<tr>
<td>S3-Resp</td>
<td>0.88</td>
<td>0.88</td>
<td>0.70</td>
<td>0.64</td>
<td>0.87</td>
<td>0.70</td>
<td>0.92</td>
<td>0.96</td>
<td>0.86</td>
<td>0.75</td>
<td>0.84</td>
<td>0.66</td>
</tr>
</table>

Here are the correlations to the annotations provided by Bhandari et al., (2020).
Summary-level:
<table>
<tr>
<th></th>
<th colspan="3">Bhandari2020-Abs</th>
<th colspan="3">Bhandari2020-Ext</th>
<th colspan="3">Bhandari2020-Mix</th>
</tr>
<tr>
<th></th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
</tr>
<tr>
<td>S3-Pyr</td>
<td>0.67</td>
<td>0.64</td>
<td>0.53</td>
<td>0.25</td>
<td>0.21</td>
<td>0.17</td>
<td>0.53</td>
<td>0.49</td>
<td>0.40</td>
</tr>
<tr>
<td>S3-Resp</td>
<td>0.65</td>
<td>0.62</td>
<td>0.51</td>
<td>0.25</td>
<td>0.21</td>
<td>0.18</td>
<td>0.51</td>
<td>0.48</td>
<td>0.38</td>
</tr>
</table>

System-level:
<table>
<tr>
<th></th>
<th colspan="3">Bhandari2020-Abs</th>
<th colspan="3">Bhandari2020-Ext</th>
<th colspan="3">Bhandari2020-Mix</th>
</tr>
<tr>
<th></th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
<th>r</th>
<th>p</th>
<th>k</th>
</tr>
<tr>
<td>S3-Pyr</td>
<td>0.97</td>
<td>0.87</td>
<td>0.71</td>
<td>0.70</td>
<td>0.61</td>
<td>0.51</td>
<td>0.96</td>
<td>0.95</td>
<td>0.82</td>
</tr>
<tr>
<td>S3-Resp</td>
<td>0.98</td>
<td>0.96</td>
<td>0.87</td>
<td>0.68</td>
<td>0.49</td>
<td>0.38</td>
<td>0.97</td>
<td>0.95</td>
<td>0.85</td>
</tr>
</table>

## References
[1] Maxime Peyrard, Teresa Botschen, and Iryna Gurevych. [Learning to Score System Summaries for Better Content Selection Evaluation](https://www.aclweb.org/anthology/W17-4510.pdf). Workshop on New Frontiers in Summarization, 2017.
